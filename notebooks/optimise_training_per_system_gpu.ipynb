{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c25c5da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DipeshBindlish\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Core libs for dataset, model, tokenizer, trainer\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import accelerate\n",
    "import torch\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1809d115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable Weights & Biases logging for now to reduce overhead\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eba6e0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 500)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the \"emotion\" dataset\n",
    "ds = load_dataset(\"emotion\")\n",
    "\n",
    "# Use a smaller subset for quick speed tests\n",
    "# e.g. 2000 train samples, 500 validation samples\n",
    "train_small = ds[\"train\"].select(range(2000))\n",
    "eval_small = ds[\"validation\"].select(range(500))\n",
    "\n",
    "len(train_small), len(eval_small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb721793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Use DistilBERT: lighter and faster than bert-base\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=6\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "782363d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization: text -> input_ids + attention_mask\n",
    "def tok(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "train_small_tok = train_small.map(tok, batched=True)\n",
    "eval_small_tok = eval_small.map(tok, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "787b2386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor(0),\n",
       " 'input_ids': tensor([  101,  1045,  2134,  2102,  2514, 26608,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove raw text, rename label -> labels, set torch format\n",
    "train_small_tok = train_small_tok.remove_columns([\"text\"])\n",
    "eval_small_tok = eval_small_tok.remove_columns([\"text\"])\n",
    "\n",
    "train_small_tok = train_small_tok.rename_column(\"label\", \"labels\")\n",
    "eval_small_tok = eval_small_tok.rename_column(\"label\", \"labels\")\n",
    "\n",
    "train_small_tok.set_format(type=\"torch\")\n",
    "eval_small_tok.set_format(type=\"torch\")\n",
    "\n",
    "train_small_tok[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94653151",
   "metadata": {},
   "source": [
    "# Force Use CPU for Training:\n",
    "``no_cuda=True``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45fd3a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DipeshBindlish\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:1595: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "# Small, quick run to test speed\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"text_out_debug\",          # where to save\n",
    "    per_device_train_batch_size=16,       # same as before, but on small dataset\n",
    "    num_train_epochs=1,                   # 1 epoch for now\n",
    "    logging_steps=20,                     # log often to see step timing\n",
    "    no_cuda=True                          # force CPU; keep it explicit for now\n",
    ")\n",
    "\n",
    "print(\"Using CUDA:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "964952ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 04:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.625600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.554100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.357400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.088800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 17min 48s\n",
      "Wall time: 5min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=1.3074206581115724, metrics={'train_runtime': 299.9994, 'train_samples_per_second': 6.667, 'train_steps_per_second': 0.417, 'total_flos': 33119212032000.0, 'train_loss': 1.3074206581115724, 'epoch': 1.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_small_tok,\n",
    "    eval_dataset=eval_small_tok\n",
    ")\n",
    "\n",
    "# Optional: Jupyter magic to time the cell\n",
    "%time trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b7518",
   "metadata": {},
   "source": [
    "# Use GPU if available:\n",
    "``no_cuda`` removed from params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d381469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"text_out_gpu\",\n",
    "    per_device_train_batch_size=4,   # smaller because MX450 only 2GB\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=20,\n",
    "    report_to=[],\n",
    "    # GPU allowed automatically if available\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "698826f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DipeshBindlish\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 07:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.724500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.620300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.539700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.553900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.374100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.223000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.242100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.127900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.146700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.178300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.153100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.858900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.749000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.924700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.924400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.661200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.650300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.642400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.634000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.695000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.633400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.587400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.620700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.567600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.469400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 24min 6s\n",
      "Wall time: 7min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.9800766468048095, metrics={'train_runtime': 423.887, 'train_samples_per_second': 4.718, 'train_steps_per_second': 1.18, 'total_flos': 33119212032000.0, 'train_loss': 0.9800766468048095, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_small_tok,\n",
    "    eval_dataset=eval_small_tok\n",
    ")\n",
    "\n",
    "# Optional: Jupyter magic to time the cell\n",
    "%time trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1553e504",
   "metadata": {},
   "source": [
    "# GPU took longer so let's verify if the GPU was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70cf10c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DipeshBindlish\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\cuda\\__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, None, True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available(), torch.version.cuda, torch.backends.cudnn.enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae126cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pynvml in c:\\users\\dipeshbindlish\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (13.0.1)\n",
      "Requirement already satisfied: nvidia-ml-py>=12.0.0 in c:\\users\\dipeshbindlish\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pynvml) (13.580.82)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "319fef56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute mode: 0\n",
      "Mode: WDDM (Default)\n"
     ]
    }
   ],
   "source": [
    "import pynvml\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "mode = pynvml.nvmlDeviceGetComputeMode(handle)\n",
    "print(\"Compute mode:\", mode)\n",
    "\n",
    "modes = {\n",
    "    0: \"WDDM (Default)\",\n",
    "    1: \"Exclusive\",\n",
    "    2: \"Prohibited\",\n",
    "    3: \"Exclusive Process\"\n",
    "}\n",
    "\n",
    "print(\"Mode:\", modes.get(mode, \"Unknown\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f80ebf6",
   "metadata": {},
   "source": [
    "The Current System is using WDDM which doesn't allow CUDA operations which is required. This restriction is speciifc to Windows, this same graphic crad can be used in Linux system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b6cd61",
   "metadata": {},
   "source": [
    "# So, For Windows we can use WSL for training, via creating a virtual env there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cd7772",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
