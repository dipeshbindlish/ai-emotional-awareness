{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00260ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries used for dataset, model, tokenization, and probabilities\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87153fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'i didnt feel humiliated', 'label': 0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the \"emotion\" dataset from HuggingFace\n",
    "ds = load_dataset(\"emotion\")\n",
    "\n",
    "# Print one sample to understand structure (text + label)\n",
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b78822f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.dataset_dict.DatasetDict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(ds))\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8ec021b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See label names (human readable)\n",
    "label_names = ds[\"train\"].features[\"label\"].names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4098757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'i wont let me child cry it out because i feel that loving her and lily when she was little was going to be opportunities that only lasted for those short few months',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the raw input text and its label\n",
    "sample = ds[\"train\"][100]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc179b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer (turns text → tokens → numbers)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d2313fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2180, 2102, 2292, 2033, 2775, 5390, 2009, 2041, 2138, 1045,\n",
       "         2514, 2008, 8295, 2014, 1998, 7094, 2043, 2016, 2001, 2210, 2001, 2183,\n",
       "         2000, 2022, 6695, 2008, 2069, 6354, 2005, 2216, 2460, 2261, 2706,  102,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert text into token IDs, pad/truncate to same length, return tensors\n",
    "tok = tokenizer(\n",
    "    sample[\"text\"],\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=64,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "437ddcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load BERT for emotion classification (6 labels)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4d6c08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0931,  0.2348, -0.6436,  0.0639, -0.3557, -0.1919]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send tokenized input into model (forward pass)\n",
    "outputs = model(**tok)\n",
    "\n",
    "# Raw model output (logits) before softmax\n",
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "882bced4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2003, 0.2308, 0.0959, 0.1945, 0.1279, 0.1506]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert logits to probabilities\n",
    "probs = F.softmax(outputs.logits, dim=-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c636908c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'joy'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick the index of max probability\n",
    "pred_idx = probs.argmax(dim=-1).item()\n",
    "\n",
    "# Convert to label name\n",
    "pred_label = label_names[pred_idx]\n",
    "pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "224d1ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Truth: love   |   Pred: joy'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Truth: \" + label_names[sample[\"label\"]] + \"   |   Pred: \" + pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cf42056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFlow observed:\\n1. raw text\\n2. tokenization to input_ids & mask\\n3. model forward\\n4. logits\\n5. softmax probabilities\\n6. predicted label\\n\\nNext: fine-tuning to improve predictions\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary (just a text cell maybe)\n",
    "'''\n",
    "Flow observed:\n",
    "1. raw text\n",
    "2. tokenization to input_ids & mask\n",
    "3. model forward\n",
    "4. logits\n",
    "5. softmax probabilities\n",
    "6. predicted label\n",
    "\n",
    "Next: fine-tuning to improve predictions\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
